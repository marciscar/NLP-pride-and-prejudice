{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfa108d2-e50a-4f39-9eb9-e05974307ef6",
   "metadata": {},
   "source": [
    "# Pride and prejudice text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10327d5e-b376-4db6-a8bc-209af42502aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56c4278-d7c1-4378-9e3b-a15d1aab041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0224e1d3-1e93-4fc0-bbab-a6e3a5c67f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/pride_and_prejudice.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e366b633-e94b-443f-bb4c-e42da435e453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nIt is a truth universally acknowledged, that a single man in possession\\nof a good fortune, must be in want of a wife.\\n\\nHowever little known the feelings or views of such a man may be on his\\nfirst entering a neighbourhood, this truth is so well fixed in the minds\\nof the surrounding families, that he is considered the rightful property\\nof some one or other of their daughters.\\n\\n“My dear Mr. Bennet,” said his lady to him one day, “have you heard that\\nNetherfield Park is let at last?”\\n\\nMr. Bennet replied that he had not.\\n\\n“But it is,” returned she; “for Mrs. Long has just been here, and she\\ntold me all about it.”\\n\\nMr. Bennet made no answer.\\n\\n“Do you not want to know who has taken it?” cried his wife impatiently.\\n\\n“_You_ want to tell me, and I have no objection to hearing it.”\\n\\nThis was invitation enough.\\n\\n“Why, my dear, you must know, Mrs. Long says that Netherfield is taken\\nby a young man of large fortune from the north of England; that he came\\ndown on Monday in a chaise and fo'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ded624-6668-4ce9-953d-856b0954323a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "\n",
      "\n",
      "It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune, must be in want of a wife.\n",
      "\n",
      "However little known the feelings or views of such a man may be on his\n",
      "first entering a neighbourhood, this truth is so well fixed in the minds\n",
      "of the surrounding families, that he is considered the rightful property\n",
      "of some one or other of their daughters.\n",
      "\n",
      "“My dear Mr. Bennet,” said his lady to him one day, “have you heard that\n",
      "Netherfield Park is let at last?”\n",
      "\n",
      "Mr. Bennet replied that he had not.\n",
      "\n",
      "“But it is,” returned she; “for Mrs. Long has just been here, and she\n",
      "told me all about it.”\n",
      "\n",
      "Mr. Bennet made no answer.\n",
      "\n",
      "“Do you not want to know who has taken it?” cried his wife impatiently.\n",
      "\n",
      "“_You_ want to tell me, and I have no objection to hearing it.”\n",
      "\n",
      "This was invitation enough.\n",
      "\n",
      "“Why, my dear, you must know, Mrs. Long says that Netherfield is taken\n",
      "by a young man of large fortune from the north of England; that he came\n",
      "down on Monday in a chaise and fo\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8758aae-1642-496c-98a2-2290cbd23b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684743"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27bf277-8f40-4507-9f16-bf8b5ccfecd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Encode entire text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d350d8-d003-4a2a-b2f6-fc18e057d2b0",
   "metadata": {},
   "source": [
    "We create an encoder and a decoder for each character in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ba77f7-774a-4453-8fdf-5e4f4542730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = sorted(list(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d477247-bcea-4bb3-945d-8a23deade064",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = dict(enumerate(all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59bd2795-535d-416b-8b9e-cc42947d7c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(0, '\\n'), (1, ' '), (2, '!'), (3, \"'\"), (4, '('), (5, ')'), (6, '*'), (7, ','), (8, '-'), (9, '.'), (10, '0'), (11, '1'), (12, '2'), (13, '3'), (14, '4'), (15, '5'), (16, '6'), (17, '7'), (18, '8'), (19, '9'), (20, ':'), (21, ';'), (22, '?'), (23, 'A'), (24, 'B'), (25, 'C'), (26, 'D'), (27, 'E'), (28, 'F'), (29, 'G'), (30, 'H'), (31, 'I'), (32, 'J'), (33, 'K'), (34, 'L'), (35, 'M'), (36, 'N'), (37, 'O'), (38, 'P'), (39, 'R'), (40, 'S'), (41, 'T'), (42, 'U'), (43, 'V'), (44, 'W'), (45, 'Y'), (46, 'Z'), (47, '_'), (48, 'a'), (49, 'b'), (50, 'c'), (51, 'd'), (52, 'e'), (53, 'f'), (54, 'g'), (55, 'h'), (56, 'i'), (57, 'j'), (58, 'k'), (59, 'l'), (60, 'm'), (61, 'n'), (62, 'o'), (63, 'p'), (64, 'q'), (65, 'r'), (66, 's'), (67, 't'), (68, 'u'), (69, 'v'), (70, 'w'), (71, 'x'), (72, 'y'), (73, 'z'), (74, '“'), (75, '”')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f09e2d9b-920b-4937-a870-2dacd5437a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {char: ind for ind,char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "682e9661-b897-46d9-834a-59818f8caad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 55, 48, 63, 67, 52, 65,  1, 11,  0,  0,  0, 31, 67,  1, 56, 66,\n",
       "        1, 48,  1, 67, 65, 68, 67, 55,  1, 68, 61, 56, 69, 52, 65, 66, 48,\n",
       "       59, 59, 72,  1, 48, 50, 58, 61, 62, 70, 59, 52, 51, 54, 52, 51,  7,\n",
       "        1, 67, 55, 48, 67,  1, 48,  1, 66, 56, 61, 54, 59, 52,  1, 60, 48,\n",
       "       61,  1, 56, 61,  1, 63, 62, 66, 66, 52, 66, 66, 56, 62, 61,  0, 62,\n",
       "       53,  1, 48,  1, 54, 62, 62, 51,  1, 53, 62, 65, 67, 68, 61])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])\n",
    "encoded_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "221ebc13-28a1-431a-a43d-e46658a0d654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef333bd-08d0-468c-b7d2-984965f66c9e",
   "metadata": {},
   "source": [
    "**We one-hot encode our data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1e3ad30-7b54-47a4-947d-e211f9a88035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    '''\n",
    "    encoded_text : batch of encoded text\n",
    "    \n",
    "    num_uni_chars = number of unique characters (len(set(text)))\n",
    "    '''\n",
    "    \n",
    "    # METHOD FROM:\n",
    "    # https://stackoverflow.com/questions/29831489/convert-encoded_textay-of-indices-to-1-hot-encoded-numpy-encoded_textay\n",
    "      \n",
    "    # Create a placeholder for zeros.\n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "    \n",
    "    # Convert data type for later use with pytorch\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "\n",
    "    # Using fancy indexing fill in the 1s at the correct index locations\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "    \n",
    "    # Reshape it so it matches the batch shape\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b183e9c-fbd7-4d54-9944-5365fc3880aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_encoder(np.array([1,2,0]),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be751f1-0e6c-4c13-aab4-87b8ea37f173",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create the training batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d530075-68f0-46af-901e-62ce89f7887e",
   "metadata": {},
   "source": [
    "We create a function that will generate batches of characters along with the next character in the sequence as a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a2f37f6-f9c4-4bb3-8f81-59677c22636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    \n",
    "    '''\n",
    "    Generate batches for training.\n",
    "    \n",
    "    X: Encoded Text of length seq_len\n",
    "    Y: Encoded Text shifted by one\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    x:\n",
    "    \n",
    "    [[1 2 3]]\n",
    "    \n",
    "    y:\n",
    "    \n",
    "    [[2 3 4]]\n",
    "    \n",
    "    encoded_text : Complete Encoded Text to make batches from\n",
    "    samp_per_batch : Number of samples (sequences) per batch\n",
    "    seq_len : Length of character sequence\n",
    "       \n",
    "    '''\n",
    "    \n",
    "    # Total number of characters per batch:\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "    \n",
    "    # Number of batches available to make (rounded down to integer)\n",
    "    num_batches_avail = int(np.floor(len(encoded_text)/char_per_batch))\n",
    "    \n",
    "    # Cut off end of encoded_text that won't fit evenly into a batch\n",
    "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "    \n",
    "    # Reshape text into samp_per_batch rows\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "    \n",
    "    # Go through each row in array.\n",
    "    for n in range(0, encoded_text.shape[1], seq_len):\n",
    "        \n",
    "        # Grab feature characters\n",
    "        x = encoded_text[:, n:n+seq_len]\n",
    "        \n",
    "        # Go through each row in array.\n",
    "        y = np.zeros_like(x)\n",
    "       \n",
    "        try:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
    "            \n",
    "        # End of the row:    \n",
    "        except:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, 0]\n",
    "            \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb59ab-b25e-4946-b7bc-b05b94b02993",
   "metadata": {},
   "source": [
    "### Example of generating a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cfdf76e-fb2b-4d9f-8186-46ea3db86018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 55, 48, 63, 67, 52, 65,  1, 11,  0,  0,  0, 31, 67,  1, 56, 66,\n",
       "        1, 48,  1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = encoded_text[:20]\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10ece794-3a53-4362-8099-69784df2a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(sample_text, samp_per_batch=2, seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69b2a7fe-4721-47d1-8137-5a98770dae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab first batch\n",
    "x, y = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bada9009-422e-487c-a3ba-31041a33db0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25, 55, 48, 63, 67],\n",
       "       [ 0,  0, 31, 67,  1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5724994-659e-4d5f-af4e-40ab038027c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55, 48, 63, 67, 52],\n",
       "       [ 0, 31, 67,  1, 56]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30192a9c-a008-4058-a532-77b2ed01d995",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Creating the LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e77b1bdf-2943-4890-8785-69b3daeeca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4, drop_prob=0.5, use_gpu=False):\n",
    "        \n",
    "        \n",
    "        # SET UP ATTRIBUTES\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        #CHARACTER SET, ENCODER, and DECODER\n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "                \n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        return final_out, hidden\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        '''\n",
    "        Used as separate method to account for both GPU and CPU users.\n",
    "        '''\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a443be-7ba2-49ff-bfae-088158d60d53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Instance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5b8ae13-1397-4cee-8137-4da6506e7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=256,\n",
    "    num_layers=2,\n",
    "    drop_prob=0.3,\n",
    "    use_gpu=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00ded145-6c39-4f2d-b317-d7f3999c24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_param  = []\n",
    "for p in model.parameters():\n",
    "    total_param.append(int(p.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f52432e8-f609-4dbe-afd0-2ed29a62b198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "887884"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1211c223-10c2-428d-a48d-891a327af2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684743"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1231fc-6aa7-44b7-9885-ab8f2f3d5b4f",
   "metadata": {},
   "source": [
    "The number of parameters is roughly of the same magnitude of the total number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e6170cf-6fb9-4b3b-b6f0-a93099eda565",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.002)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78511e3-50db-43ae-9976-ccfd9b5f93c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training Data and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc709a28-b6dd-480e-9fa3-967b5270f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = int(len(encoded_text)*0.8)\n",
    "\n",
    "train_data = encoded_text[:train_length]\n",
    "val_data = encoded_text[train_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f508c3c-0114-4ccb-a957-39e62c624db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs to train for\n",
    "epochs = 80\n",
    "\n",
    "# batch size \n",
    "batch_size = 128\n",
    "\n",
    "# Length of sequence\n",
    "seq_len = 200\n",
    "\n",
    "tracker = 0\n",
    "\n",
    "# number of characters in text\n",
    "num_char = max(encoded_text) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8f7bf7d-ee5d-4a25-8f39-aab0415aa682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 25 Val Loss: 3.100309133529663\n",
      "Epoch: 2 Step: 50 Val Loss: 3.093801498413086\n",
      "Epoch: 3 Step: 75 Val Loss: 3.084357261657715\n",
      "Epoch: 4 Step: 100 Val Loss: 3.0183935165405273\n",
      "Epoch: 5 Step: 125 Val Loss: 2.7930691242218018\n",
      "Epoch: 7 Step: 150 Val Loss: 2.6292505264282227\n",
      "Epoch: 8 Step: 175 Val Loss: 2.507021903991699\n",
      "Epoch: 9 Step: 200 Val Loss: 2.4266793727874756\n",
      "Epoch: 10 Step: 225 Val Loss: 2.367846727371216\n",
      "Epoch: 11 Step: 250 Val Loss: 2.286653995513916\n",
      "Epoch: 13 Step: 275 Val Loss: 2.2247824668884277\n",
      "Epoch: 14 Step: 300 Val Loss: 2.1715362071990967\n",
      "Epoch: 15 Step: 325 Val Loss: 2.1219828128814697\n",
      "Epoch: 16 Step: 350 Val Loss: 2.0771384239196777\n",
      "Epoch: 17 Step: 375 Val Loss: 2.033853530883789\n",
      "Epoch: 19 Step: 400 Val Loss: 1.9908596277236938\n",
      "Epoch: 20 Step: 425 Val Loss: 1.9494863748550415\n",
      "Epoch: 21 Step: 450 Val Loss: 1.912807822227478\n",
      "Epoch: 22 Step: 475 Val Loss: 1.8778191804885864\n",
      "Epoch: 23 Step: 500 Val Loss: 1.842667579650879\n",
      "Epoch: 24 Step: 525 Val Loss: 1.8127238750457764\n",
      "Epoch: 26 Step: 550 Val Loss: 1.786888837814331\n",
      "Epoch: 27 Step: 575 Val Loss: 1.7604271173477173\n",
      "Epoch: 28 Step: 600 Val Loss: 1.7343707084655762\n",
      "Epoch: 29 Step: 625 Val Loss: 1.7219302654266357\n",
      "Epoch: 30 Step: 650 Val Loss: 1.6926567554473877\n",
      "Epoch: 32 Step: 675 Val Loss: 1.6726115942001343\n",
      "Epoch: 33 Step: 700 Val Loss: 1.6553024053573608\n",
      "Epoch: 34 Step: 725 Val Loss: 1.6373190879821777\n",
      "Epoch: 35 Step: 750 Val Loss: 1.6227701902389526\n",
      "Epoch: 36 Step: 775 Val Loss: 1.605218529701233\n",
      "Epoch: 38 Step: 800 Val Loss: 1.5904889106750488\n",
      "Epoch: 39 Step: 825 Val Loss: 1.582424521446228\n",
      "Epoch: 40 Step: 850 Val Loss: 1.5657521486282349\n",
      "Epoch: 41 Step: 875 Val Loss: 1.55413818359375\n",
      "Epoch: 42 Step: 900 Val Loss: 1.5416334867477417\n",
      "Epoch: 44 Step: 925 Val Loss: 1.528684377670288\n",
      "Epoch: 45 Step: 950 Val Loss: 1.5183134078979492\n",
      "Epoch: 46 Step: 975 Val Loss: 1.5084130764007568\n",
      "Epoch: 47 Step: 1000 Val Loss: 1.4971624612808228\n",
      "Epoch: 48 Step: 1025 Val Loss: 1.4924490451812744\n",
      "Epoch: 49 Step: 1050 Val Loss: 1.4783746004104614\n",
      "Epoch: 51 Step: 1075 Val Loss: 1.4683414697647095\n",
      "Epoch: 52 Step: 1100 Val Loss: 1.4620938301086426\n",
      "Epoch: 53 Step: 1125 Val Loss: 1.4532265663146973\n",
      "Epoch: 54 Step: 1150 Val Loss: 1.4495919942855835\n",
      "Epoch: 55 Step: 1175 Val Loss: 1.438836693763733\n",
      "Epoch: 57 Step: 1200 Val Loss: 1.4373565912246704\n",
      "Epoch: 58 Step: 1225 Val Loss: 1.427446722984314\n",
      "Epoch: 59 Step: 1250 Val Loss: 1.4222809076309204\n",
      "Epoch: 60 Step: 1275 Val Loss: 1.412338137626648\n",
      "Epoch: 61 Step: 1300 Val Loss: 1.4071061611175537\n",
      "Epoch: 63 Step: 1325 Val Loss: 1.3996847867965698\n",
      "Epoch: 64 Step: 1350 Val Loss: 1.3935946226119995\n",
      "Epoch: 65 Step: 1375 Val Loss: 1.3892321586608887\n",
      "Epoch: 66 Step: 1400 Val Loss: 1.3836489915847778\n",
      "Epoch: 67 Step: 1425 Val Loss: 1.3795194625854492\n",
      "Epoch: 69 Step: 1450 Val Loss: 1.3726528882980347\n",
      "Epoch: 70 Step: 1475 Val Loss: 1.3675179481506348\n",
      "Epoch: 71 Step: 1500 Val Loss: 1.364830493927002\n",
      "Epoch: 72 Step: 1525 Val Loss: 1.3603222370147705\n",
      "Epoch: 73 Step: 1550 Val Loss: 1.3573368787765503\n",
      "Epoch: 74 Step: 1575 Val Loss: 1.350558876991272\n",
      "Epoch: 76 Step: 1600 Val Loss: 1.3464359045028687\n",
      "Epoch: 77 Step: 1625 Val Loss: 1.3420748710632324\n",
      "Epoch: 78 Step: 1650 Val Loss: 1.3356671333312988\n",
      "Epoch: 79 Step: 1675 Val Loss: 1.3343814611434937\n"
     ]
    }
   ],
   "source": [
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "\n",
    "# Check to see if using GPU\n",
    "if model.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    \n",
    "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
    "        \n",
    "        tracker += 1\n",
    "        \n",
    "        # One Hot Encode incoming data\n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        # Adjust for GPU if necessary\n",
    "        \n",
    "        if model.use_gpu:\n",
    "            \n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        # Reset Hidden State\n",
    "        # If we dont' reset we would backpropagate through all training history\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output, hidden = model.forward(inputs,hidden)\n",
    "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
    "        # LET\"S CLIP JUST IN CASE\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ### CHECK ON VALIDATION SET ######\n",
    "        #################################\n",
    "        \n",
    "        if tracker % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
    "                \n",
    "                # One Hot Encode incoming data\n",
    "                x = one_hot_encoder(x,num_char)\n",
    "                \n",
    "\n",
    "                # Convert Numpy Arrays to Tensor\n",
    "\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "\n",
    "                # Adjust for GPU if necessary\n",
    "\n",
    "                if model.use_gpu:\n",
    "\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                    \n",
    "                # Reset Hidden State\n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
    "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Reset to training model after val for loop\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9c5469-192d-4be9-afa5-c9a3d753a465",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Saving the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c86db026-1348-4220-a099-1db20d55cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LSTM_prideandprejudice_256_2_0.3.net'\n",
    "\n",
    "torch.save(model.state_dict(), '../models/'+model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19721fb-d4bd-4e2b-89ec-d4c0883e3784",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Loading the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df345f05-edd7-4708-a807-1176cd151b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharModel(\n",
       "  (lstm): LSTM(76, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc_linear): Linear(in_features=256, out_features=76, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Must match the same parameters as in training\n",
    "\n",
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=256,\n",
    "    num_layers=2,\n",
    "    drop_prob=0.3,\n",
    "    use_gpu=False,\n",
    ")\n",
    "\n",
    "model_name = 'LSTM_prideandprejudice_256_2_0.3.net'\n",
    "\n",
    "model.load_state_dict(torch.load('../models/'+model_name))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb72bd-d575-4474-bd1b-e7f6c288a9f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "374f0fcc-73a5-492e-88d5-5da0896615ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1, temperature=1.0):\n",
    "        \n",
    "        # Encode -> one-hot -> tensor\n",
    "    \n",
    "        encoded_text = model.encoder[char]\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "    \n",
    "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "    \n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "        \n",
    "        # Check for CPU\n",
    "        if(model.use_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # Grab hidden states\n",
    "        # hidden = tuple([state.data for state in hidden])\n",
    "        hidden = tuple(h.detach() for h in hidden)\n",
    "        \n",
    "        # Run model and get predicted output\n",
    "        lstm_out, hidden = model(inputs, hidden)\n",
    "\n",
    "    \n",
    "        # scale logits before softmax: lower temp = more conservative, higher = more random\n",
    "        if temperature <= 0:\n",
    "            temperature = 1e-8  # avoid division by zero\n",
    "        lstm_out = lstm_out / temperature\n",
    "\n",
    "\n",
    "    \n",
    "        # Convert lstm_out to probabilities\n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "        \n",
    "        if(model.use_gpu):\n",
    "            # move back to CPU to use with numpy\n",
    "            probs = probs.cpu()\n",
    "\n",
    "        # Return k largest probabilities in tensor\n",
    "        probs, index_positions = probs.topk(k)\n",
    "         \n",
    "        index_positions = index_positions.numpy().reshape(-1)\n",
    "        \n",
    "        # Create array of probabilities\n",
    "        probs = probs.numpy().reshape(-1)\n",
    "        \n",
    "        # Convert to probabilities per index\n",
    "        probs = probs/probs.sum()\n",
    "        \n",
    "        # randomly choose a character based on probabilities\n",
    "        if k==1:\n",
    "            char = int(index_positions[0])\n",
    "        else:\n",
    "            char = np.random.choice(index_positions, p=probs)\n",
    "       \n",
    "        # return the decoded value of the predicted char and the hidden state\n",
    "        return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4da1e5fe-9b3f-403a-a7d6-3a1fa1e97aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed='The', k=1, temperature=1.0):\n",
    "        \n",
    "    # CHECK FOR GPU\n",
    "    if(model.use_gpu):\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "    \n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # begin output from initial seed\n",
    "    output_chars = [c for c in seed]\n",
    "    \n",
    "    # intiate hidden state\n",
    "    hidden = model.hidden_state(1)\n",
    "    \n",
    "    # predict the next character for every character in seed\n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model, char, hidden, k=k, temperature=temperature)\n",
    "    \n",
    "    # add initial characters to output\n",
    "    output_chars.append(char)\n",
    "    \n",
    "    # Now generate for size requested\n",
    "    for i in range(size):\n",
    "        \n",
    "        # predict based off very last letter in output_chars\n",
    "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k, temperature=temperature)\n",
    "        \n",
    "        # add predicted character\n",
    "        output_chars.append(char)\n",
    "    \n",
    "    # return string of predicted text\n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c70fe-dd6d-4eef-8e89-706afe10d6a2",
   "metadata": {},
   "source": [
    "## Example generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9d21bef-47ec-478b-80bf-d50952d3b673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr.\n",
      "Bennet, was not a streat of the passing a servant was no make her to be something a day, and soon something to be some accamily the discovery as so seemed to the suppressed himself to be a family as had no seemed on the particularly, as she, “and his\n",
      "care on the chaice as he could not have been served, as to this compleshed and such opinion of a family,” replied her, when they having a manner on her family, and and sense of the caming herself.”\n",
      "\n",
      "“As she was\n",
      "not assisting the most of her\n",
      "astermined. To had been to to the sertion of the consideration which she had not befured them, that they had not been a dingryer that the provised\n",
      "her thought his settliched his accouns a day and a most been as to be, and all her from\n",
      "anything offer of the part a most day.\n",
      "\n",
      "“The prising him at a moners at her acterraining to happiness of the present attention to the same on the caming her to the provession,\n",
      "and she\n",
      "could he was soon supposing\n",
      "and such a few\n",
      "ander and a sense of her family, than think o\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='Mr', k=3, temperature=1.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
